{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12289446,"sourceType":"datasetVersion","datasetId":3442424}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ü¶ã Butterfly Species Classification - Multi-Model Training\n\n**Train & Compare 4 State-of-the-Art Models:**\n1. üèõÔ∏è **VGG16** - Classic deep architecture (138M params)\n2. üîó **ResNet50** - Residual connections (25M params)\n3. ‚ö° **EfficientNetB0** - Compound scaling (5.3M params)\n4. üì± **MobileNetV2** - Lightweight (3.5M params)\n\n**Automatic Model Selection:** The notebook will train all 4 models and automatically select the best one!\n\n**Expected Time:** ~100-120 minutes on Kaggle T4 GPU\n\n**Expected Accuracy:** 85-88% (best model)\n\n---","metadata":{}},{"cell_type":"markdown","source":"## üì¶ Step 1: Import Libraries\n\n**No installations needed!** Kaggle has everything pre-installed.\n\nThose CUDA warnings are normal - just ignore them!","metadata":{}},{"cell_type":"code","source":"# It will remove the kaggle/working directory\n# !rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:34:02.748284Z","iopub.execute_input":"2026-01-08T12:34:02.748551Z","iopub.status.idle":"2026-01-08T12:34:02.870553Z","shell.execute_reply.started":"2026-01-08T12:34:02.748520Z","shell.execute_reply":"2026-01-08T12:34:02.869690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import all required libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0, MobileNetV2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\n\nimport os\nimport json\nimport time\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Display environment info\nprint(\"=\" * 70)\nprint(\"ENVIRONMENT CHECK\")\nprint(\"=\" * 70)\nprint(f\"TensorFlow: {tf.__version__}\")\nprint(f\"Keras: {keras.__version__}\")\nprint(f\"NumPy: {np.__version__}\")\nprint(f\"Pandas: {pd.__version__}\")\nprint(f\"sklearn: {sklearn.__version__}\")\nfrom platform import python_version\n# print(python_version())\nprint(f\"Python: {python_version()}\")\n\nprint(f\"\\nGPU Devices: {tf.config.list_physical_devices('GPU')}\")\nprint(\"=\"*70)\nprint(\"=\" * 70)\nprint(\"\\n‚úÖ All libraries loaded successfully!\")\nprint(\"‚úÖ Ready to train 4 models!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:34:14.659493Z","iopub.execute_input":"2026-01-08T12:34:14.660176Z","iopub.status.idle":"2026-01-08T12:34:39.188578Z","shell.execute_reply.started":"2026-01-08T12:34:14.660126Z","shell.execute_reply":"2026-01-08T12:34:39.187753Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìÇ Step 2: Load Dataset\n\nMake sure you've added the butterfly dataset to your notebook!","metadata":{}},{"cell_type":"code","source":"# Configure paths (Kaggle format)\nDATASET_PATH = '/kaggle/input/butterfly-image-classification'\nCSV_FILE = os.path.join(DATASET_PATH, 'Training_set.csv')\nIMAGES_DIR = os.path.join(DATASET_PATH, 'train')\n\n# Verify dataset exists\nprint(\"Checking dataset...\")\nprint(f\"CSV exists: {os.path.exists(CSV_FILE)}\")\nprint(f\"Images dir exists: {os.path.exists(IMAGES_DIR)}\")\n\nif not os.path.exists(CSV_FILE):\n    raise FileNotFoundError(\n        \"Dataset not found! Please add 'butterfly-image-classification' dataset in Kaggle.\"\n    )\n\n# Load dataset\nprint(\"\\nLoading dataset...\")\ndf = pd.read_csv(CSV_FILE)\ndf['filepath'] = df['filename'].apply(lambda x: os.path.join(IMAGES_DIR, x))\n\n# Verify all files exist\nexisting_files = df['filepath'].apply(os.path.exists)\nprint(f\"Files found: {existing_files.sum()}/{len(df)} ({existing_files.sum()/len(df)*100:.1f}%)\")\n\nif not all(existing_files):\n    print(f\"‚ö†Ô∏è Warning: {(~existing_files).sum()} files missing. Removing from dataset.\")\n    df = df[existing_files].reset_index(drop=True)\n\n# Dataset statistics\nprint(f\"\\n{'='*70}\")\nprint(\"DATASET STATISTICS\")\nprint(f\"{'='*70}\")\nprint(f\"Total images: {len(df)}\")\nprint(f\"Number of species: {df['label'].nunique()}\")\nprint(f\"Images per species (avg): {len(df) / df['label'].nunique():.1f}\")\n\n# Class distribution\nclass_counts = df['label'].value_counts()\nprint(f\"\\nClass balance:\")\nprint(f\"  Min: {class_counts.min()} images\")\nprint(f\"  Max: {class_counts.max()} images\")\nprint(f\"  Mean: {class_counts.mean():.1f} images\")\nprint(f\"{'='*70}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:36:26.933516Z","iopub.execute_input":"2026-01-08T12:36:26.934820Z","iopub.status.idle":"2026-01-08T12:36:44.596301Z","shell.execute_reply.started":"2026-01-08T12:36:26.934780Z","shell.execute_reply":"2026-01-08T12:36:44.595618Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ‚úÇÔ∏è Step 3: Train/Validation Split\n\nUsing 80/20 split with stratification to maintain class balance.","metadata":{}},{"cell_type":"code","source":"# Create stratified split\nprint(\"Creating train/validation split...\")\ntrain_df, val_df = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df['label'],\n    random_state=42  # Fixed seed for reproducibility\n)\n\nprint(f\"\\n‚úÖ Split created:\")\nprint(f\"Training set: {len(train_df)} images ({len(train_df)/len(df)*100:.1f}%)\")\nprint(f\"Validation set: {len(val_df)} images ({len(val_df)/len(df)*100:.1f}%)\")\n\n# Create class indices mapping\nunique_labels = sorted(df['label'].unique())\nclass_indices = {label: idx for idx, label in enumerate(unique_labels)}\nnum_classes = len(class_indices)\n\nprint(f\"\\nNumber of classes: {num_classes}\")\nprint(f\"Classes will be saved to 'class_indices.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:37:12.490416Z","iopub.execute_input":"2026-01-08T12:37:12.491151Z","iopub.status.idle":"2026-01-08T12:37:12.515899Z","shell.execute_reply.started":"2026-01-08T12:37:12.491123Z","shell.execute_reply":"2026-01-08T12:37:12.515260Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üñºÔ∏è Step 4: Create Data Generators\n\nTraining data will be augmented to improve generalization.","metadata":{}},{"cell_type":"code","source":"# Configuration\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\n\nprint(f\"Image size: {IMG_SIZE}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\n\n# Training data generator with augmentation\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    zoom_range=0.2,\n    shear_range=0.15,\n    fill_mode='nearest'\n)\n\n# Validation data generator (no augmentation)\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n# Create generators\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_df,\n    x_col='filepath',\n    y_col='label',\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True,\n    seed=42\n)\n\nval_generator = val_datagen.flow_from_dataframe(\n    val_df,\n    x_col='filepath',\n    y_col='label',\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False\n)\n\nprint(f\"\\n‚úÖ Data generators created:\")\nprint(f\"Training batches: {len(train_generator)}\")\nprint(f\"Validation batches: {len(val_generator)}\")\nprint(f\"Steps per epoch: {len(train_generator)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:37:20.987163Z","iopub.execute_input":"2026-01-08T12:37:20.987859Z","iopub.status.idle":"2026-01-08T12:37:27.945036Z","shell.execute_reply.started":"2026-01-08T12:37:20.987830Z","shell.execute_reply":"2026-01-08T12:37:27.944402Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üèóÔ∏è Step 5: Define Model Builders\n\nFunctions to build each of the 4 architectures.","metadata":{}},{"cell_type":"code","source":"def build_vgg16(num_classes):\n    \"\"\"VGG16: Classic deep architecture, reliable baseline\"\"\"\n    base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    base.trainable = False\n    \n    model = models.Sequential([\n        base,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.3),\n        layers.Dense(num_classes, activation='softmax')\n    ], name='VGG16')\n    \n    return model, base\n\ndef build_resnet50(num_classes):\n    \"\"\"ResNet50: Residual connections, good accuracy\"\"\"\n    base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    base.trainable = False\n    \n    model = models.Sequential([\n        base,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.3),\n        layers.Dense(num_classes, activation='softmax')\n    ], name='ResNet50')\n    \n    return model, base\n\ndef build_efficientnet(num_classes):\n    \"\"\"EfficientNetB0: State-of-the-art efficiency, often wins!\"\"\"\n    base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    base.trainable = False\n    \n    model = models.Sequential([\n        base,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.3),\n        layers.Dense(num_classes, activation='softmax')\n    ], name='EfficientNetB0')\n    \n    return model, base\n\ndef build_mobilenet(num_classes):\n    \"\"\"MobileNetV2: Lightweight and fast, good for deployment\"\"\"\n    base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    base.trainable = False\n    \n    model = models.Sequential([\n        base,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.3),\n        layers.Dense(num_classes, activation='softmax')\n    ], name='MobileNetV2')\n    \n    return model, base\n\nprint(\"‚úÖ Model builders defined:\")\nprint(\"  1. VGG16 - Classic deep CNN\")\nprint(\"  2. ResNet50 - Residual learning\")\nprint(\"  3. EfficientNetB0 - Compound scaling\")\nprint(\"  4. MobileNetV2 - Lightweight\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:38:08.124496Z","iopub.execute_input":"2026-01-08T12:38:08.124784Z","iopub.status.idle":"2026-01-08T12:38:08.137344Z","shell.execute_reply.started":"2026-01-08T12:38:08.124760Z","shell.execute_reply":"2026-01-08T12:38:08.136745Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üéØ Step 6: Define Training Function\n\nTwo-phase training: Transfer learning + Fine-tuning","metadata":{}},{"cell_type":"code","source":"def train_model(model, base_model, model_name, train_gen, val_gen):\n    \"\"\"\n    Train a model with two phases:\n    Phase 1: Transfer learning (base frozen)\n    Phase 2: Fine-tuning (last layers unfrozen)\n    \"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"TRAINING: {model_name}\")\n    print(f\"{'='*70}\")\n    print(f\"Total parameters: {model.count_params():,}\")\n    \n    start_time = time.time()\n    \n    # Phase 1: Transfer Learning\n    print(f\"\\n--- Phase 1: Transfer Learning (base frozen) ---\")\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    callbacks = [\n        EarlyStopping(\n            monitor='val_loss',\n            patience=8,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3,\n            min_lr=1e-7,\n            verbose=1\n        )\n    ]\n    \n    history1 = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=20,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Phase 2: Fine-tuning\n    print(f\"\\n--- Phase 2: Fine-tuning (last 4 layers unfrozen) ---\")\n    base_model.trainable = True\n    for layer in base_model.layers[:-4]:\n        layer.trainable = False\n    \n    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    history2 = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=10,\n        callbacks=callbacks,\n        initial_epoch=len(history1.history['loss']),\n        verbose=1\n    )\n    \n    train_time = time.time() - start_time\n    \n    # Final Evaluation\n    print(f\"\\n--- Final Evaluation ---\")\n    val_gen.reset()\n    val_loss, val_acc = model.evaluate(val_gen, verbose=0)\n    \n    # Calculate F1-score\n    val_gen.reset()\n    predictions = model.predict(val_gen, verbose=0)\n    y_pred = np.argmax(predictions, axis=1)\n    y_true = val_gen.classes\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    # Combine histories (handle case where Phase 2 stops immediately)\n    history = {\n        'loss': history1.history['loss'] + (history2.history.get('loss', [])),\n        'val_loss': history1.history['val_loss'] + (history2.history.get('val_loss', [])),\n        'accuracy': history1.history['accuracy'] + (history2.history.get('accuracy', [])),\n        'val_accuracy': history1.history['val_accuracy'] + (history2.history.get('val_accuracy', []))\n    }\n    \n    results = {\n        'model_name': model_name,\n        'val_accuracy': float(val_acc),\n        'val_loss': float(val_loss),\n        'f1_score': float(f1),\n        'train_time_minutes': train_time / 60,\n        'total_params': int(model.count_params()),\n        'history': history\n    }\n    \n    print(f\"\\n‚úÖ {model_name} Training Complete!\")\n    print(f\"   Accuracy: {val_acc*100:.2f}%\")\n    print(f\"   F1-Score: {f1:.4f}\")\n    print(f\"   Loss: {val_loss:.4f}\")\n    print(f\"   Time: {train_time/60:.1f} minutes\")\n    print(f\"{'='*70}\\n\")\n    \n    return model, results\n\nprint(\"‚úÖ Training function ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:38:21.051886Z","iopub.execute_input":"2026-01-08T12:38:21.052680Z","iopub.status.idle":"2026-01-08T12:38:21.063485Z","shell.execute_reply.started":"2026-01-08T12:38:21.052639Z","shell.execute_reply":"2026-01-08T12:38:21.062898Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üöÄ Step 7: Train All 4 Models\n\n**This will take ~100-120 minutes total!**","metadata":{}},{"cell_type":"code","source":"# Define models to train\nmodels_to_train = [\n    ('VGG16', build_vgg16),\n    ('ResNet50', build_resnet50),\n    ('EfficientNetB0', build_efficientnet),\n    ('MobileNetV2', build_mobilenet)\n]\n\n# Storage for results\nall_results = []\ntrained_models = {}\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"STARTING MULTI-MODEL TRAINING\")\nprint(\"=\"*70)\nprint(f\"Models to train: {len(models_to_train)}\")\nprint(f\"Estimated time: {len(models_to_train) * 25}-{len(models_to_train) * 30} minutes\")\nprint(f\"Start time: {datetime.now().strftime('%H:%M:%S')}\")\nprint(\"=\"*70 + \"\\n\")\n\ntotal_start = time.time()\n\n# Train each model\nfor idx, (name, builder_func) in enumerate(models_to_train, 1):\n    print(f\"\\nüîÑ [{idx}/{len(models_to_train)}] Starting {name}...\")\n    print(f\"Current time: {datetime.now().strftime('%H:%M:%S')}\")\n    \n    # Build model\n    model, base_model = builder_func(num_classes)\n    \n    # Train model\n    trained_model, results = train_model(\n        model, base_model, name,\n        train_generator, val_generator\n    )\n    \n    # Store results\n    all_results.append(results)\n    trained_models[name] = trained_model\n    \n    # Progress update\n    elapsed = (time.time() - total_start) / 60\n    remaining = (len(models_to_train) - idx) * (elapsed / idx)\n    print(f\"\\nüìä Progress: {idx}/{len(models_to_train)} complete\")\n    print(f\"‚è±Ô∏è  Elapsed: {elapsed:.1f} min | Estimated remaining: {remaining:.1f} min\")\n    print(f\"Estimated completion: {(datetime.now() + pd.Timedelta(minutes=remaining)).strftime('%H:%M:%S')}\")\n\ntotal_time = (time.time() - total_start) / 60\nprint(f\"\\n{'='*70}\")\nprint(\"üéâ ALL MODELS TRAINED!\")\nprint(f\"{'='*70}\")\nprint(f\"Total training time: {total_time:.1f} minutes ({total_time/60:.1f} hours)\")\nprint(f\"Completion time: {datetime.now().strftime('%H:%M:%S')}\")\nprint(f\"{'='*70}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T12:38:25.667380Z","iopub.execute_input":"2026-01-08T12:38:25.667928Z","iopub.status.idle":"2026-01-08T14:12:43.498663Z","shell.execute_reply.started":"2026-01-08T12:38:25.667902Z","shell.execute_reply":"2026-01-08T14:12:43.497966Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìä Step 8: Compare Results & Select Best Model\n\nComparing all 4 models to find the winner!","metadata":{}},{"cell_type":"code","source":"# Create comparison DataFrame\ncomparison_df = pd.DataFrame([{\n    'Model': r['model_name'],\n    'Accuracy (%)': r['val_accuracy'] * 100,\n    'F1-Score': r['f1_score'],\n    'Loss': r['val_loss'],\n    'Parameters (M)': r['total_params'] / 1e6,\n    'Time (min)': r['train_time_minutes']\n} for r in all_results])\n\n# Sort by accuracy (descending)\ncomparison_df = comparison_df.sort_values('Accuracy (%)', ascending=False).reset_index(drop=True)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL COMPARISON RESULTS\")\nprint(\"=\"*70)\nprint(comparison_df.to_string(index=False))\nprint(\"=\"*70 + \"\\n\")\n\n# Find best model\nbest_idx = comparison_df['Accuracy (%)'].idxmax()\nbest_model_name = comparison_df.loc[best_idx, 'Model']\nbest_accuracy = comparison_df.loc[best_idx, 'Accuracy (%)']\nbest_f1 = comparison_df.loc[best_idx, 'F1-Score']\nbest_params = comparison_df.loc[best_idx, 'Parameters (M)']\nbest_time = comparison_df.loc[best_idx, 'Time (min)']\n\nprint(f\"\\nüèÜ WINNER: {best_model_name}\")\nprint(f\"{'='*70}\")\nprint(f\"Accuracy: {best_accuracy:.2f}%\")\nprint(f\"F1-Score: {best_f1:.4f}\")\nprint(f\"Parameters: {best_params:.1f}M\")\nprint(f\"Training Time: {best_time:.1f} minutes\")\nprint(f\"{'='*70}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:13:35.408051Z","iopub.execute_input":"2026-01-08T14:13:35.408412Z","iopub.status.idle":"2026-01-08T14:13:35.441661Z","shell.execute_reply.started":"2026-01-08T14:13:35.408383Z","shell.execute_reply":"2026-01-08T14:13:35.440698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìà Step 9: Visualize Comparison\n\nCreate beautiful comparison charts!","metadata":{}},{"cell_type":"code","source":"# Set style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n# Create comparison charts\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\nfig.suptitle('ü¶ã Butterfly Classifier - Model Comparison', fontsize=20, fontweight='bold', y=0.995)\n\n# Colors: winner = green, others = blue\ncolors = ['#10b981' if x == best_model_name else '#3b82f6' for x in comparison_df['Model']]\n\n# 1. Accuracy comparison\nax1 = axes[0, 0]\nbars1 = ax1.bar(comparison_df['Model'], comparison_df['Accuracy (%)'], color=colors)\nax1.set_title('Validation Accuracy', fontsize=14, fontweight='bold')\nax1.set_ylabel('Accuracy (%)')\nax1.set_ylim([comparison_df['Accuracy (%)'].min() - 2, 100])\nfor bar in bars1:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\nax1.grid(axis='y', alpha=0.3)\nax1.tick_params(axis='x', rotation=45)\n\n# 2. F1-Score comparison\nax2 = axes[0, 1]\nbars2 = ax2.bar(comparison_df['Model'], comparison_df['F1-Score'], color=colors)\nax2.set_title('F1-Score', fontsize=14, fontweight='bold')\nax2.set_ylabel('F1-Score')\nax2.set_ylim([comparison_df['F1-Score'].min() - 0.02, 1.0])\nfor bar in bars2:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\nax2.grid(axis='y', alpha=0.3)\nax2.tick_params(axis='x', rotation=45)\n\n# 3. Parameters vs Accuracy scatter\nax3 = axes[1, 0]\nscatter = ax3.scatter(comparison_df['Parameters (M)'], comparison_df['Accuracy (%)'],\n                      s=300, alpha=0.6, c=colors, edgecolors='black', linewidth=2)\nfor idx, row in comparison_df.iterrows():\n    ax3.annotate(row['Model'], (row['Parameters (M)'], row['Accuracy (%)']),\n                 xytext=(5, 5), textcoords='offset points', fontweight='bold')\nax3.set_title('Model Size vs Accuracy', fontsize=14, fontweight='bold')\nax3.set_xlabel('Parameters (Millions)')\nax3.set_ylabel('Accuracy (%)')\nax3.grid(alpha=0.3)\n\n# 4. Training time comparison\nax4 = axes[1, 1]\nbars4 = ax4.bar(comparison_df['Model'], comparison_df['Time (min)'], color=colors)\nax4.set_title('Training Time', fontsize=14, fontweight='bold')\nax4.set_ylabel('Time (minutes)')\nfor bar in bars4:\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.1f}', ha='center', va='bottom', fontweight='bold')\nax4.grid(axis='y', alpha=0.3)\nax4.tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"‚úÖ Comparison chart saved as 'model_comparison.png'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:13:45.818822Z","iopub.execute_input":"2026-01-08T14:13:45.819116Z","iopub.status.idle":"2026-01-08T14:13:47.042568Z","shell.execute_reply.started":"2026-01-08T14:13:45.819090Z","shell.execute_reply":"2026-01-08T14:13:47.041877Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üíæ Step 10: Save Best Model & All Results\n\nSaving everything you need for deployment!","metadata":{}},{"cell_type":"code","source":"print(\"Saving files...\\n\")\n\n# 1. Save best model in MULTIPLE formats for maximum compatibility\nbest_model = trained_models[best_model_name]\n\nprint(\"üíæ Saving model in multiple formats...\")\nprint(\"=\"*70)\n\n# Format 1: .keras (Keras 3.x native - RECOMMENDED for Streamlit)\ntry:\n    best_model.save('butterfly_model_best.keras')\n    print(\"‚úÖ Saved: butterfly_model_best.keras (Keras 3.x native format)\")\n    model_saved = 'keras'\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  .keras format failed: {str(e)[:100]}\")\n    model_saved = 'h5'\n\n# Format 2: Weights only (.weights.h5 - Most compatible fallback)\ntry:\n    best_model.save_weights('butterfly_model_best.weights.h5')\n    print(\"‚úÖ Saved: butterfly_model_best.weights.h5 (weights only)\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Weights save failed: {str(e)[:100]}\")\n\n# Format 3: H5 format (Legacy - for backward compatibility)\ntry:\n    # Remove the deprecated save_format argument\n    best_model.save('butterfly_model_best.h5')\n    print(\"‚úÖ Saved: butterfly_model_best.h5 (H5 format)\")\n    if model_saved != 'keras':\n        model_saved = 'h5'\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  H5 format failed: {str(e)[:100]}\")\n\n# Format 4: SavedModel format (TensorFlow native - for production)\ntry:\n    best_model.export('butterfly_model_savedmodel')\n    print(\"‚úÖ Saved: butterfly_model_savedmodel/ (TensorFlow SavedModel)\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  SavedModel export failed: {str(e)[:100]}\")\n\nprint(\"=\"*70)\nprint(f\"\\nüì¶ Model saved in multiple formats!\")\nprint(f\"   Primary format: {model_saved}\")\nprint(f\"   ‚≠ê RECOMMENDED: Use 'butterfly_model_best.keras' for Streamlit!\")\nprint(\"=\"*70)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:19:13.522939Z","iopub.execute_input":"2026-01-08T14:19:13.523233Z","iopub.status.idle":"2026-01-08T14:19:20.170344Z","shell.execute_reply.started":"2026-01-08T14:19:13.523209Z","shell.execute_reply":"2026-01-08T14:19:20.169529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"=\"*70)\n# Save class indices\nclass_indices = train_generator.class_indices\nwith open('class_indices.json', 'w') as f:\n    json.dump(class_indices, f, indent=2)\nprint(f\"‚úì Class indices saved\")\nprint(\"=\"*70)\n\nprint(\"=\"*70)\n# Save model comparison\ncomparison_df.to_csv('model_comparison.csv', index=False)\nprint(f\"‚úì Model comparison saved\")\nprint(\"=\"*70)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:19:32.172521Z","iopub.execute_input":"2026-01-08T14:19:32.172819Z","iopub.status.idle":"2026-01-08T14:19:32.180081Z","shell.execute_reply.started":"2026-01-08T14:19:32.172796Z","shell.execute_reply":"2026-01-08T14:19:32.179413Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìä Step 11: Display Final Summary\n\nComplete overview of your training results!","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*70)\nprint(\"üéâ TRAINING COMPLETE - FINAL SUMMARY\")\nprint(\"=\"*70)\n\nprint(f\"\\nüìÖ Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"‚è±Ô∏è  Total Time: {total_time:.1f} minutes ({total_time/60:.2f} hours)\")\nprint(f\"üñ•Ô∏è  Environment: TensorFlow {tf.__version__}, Keras {keras.__version__}\")\nprint(f\"üéÆ GPUs Used: {len(tf.config.list_physical_devices('GPU'))}\")\n\nprint(f\"\\nüìä Dataset:\")\nprint(f\"   Total Images: {len(df)}\")\nprint(f\"   Training: {len(train_df)} images\")\nprint(f\"   Validation: {len(val_df)} images\")\nprint(f\"   Species: {num_classes}\")\n\nprint(f\"\\nüèÜ Winner: {best_model_name}\")\nprint(f\"   Accuracy: {best_accuracy:.2f}%\")\nprint(f\"   F1-Score: {best_f1:.4f}\")\nprint(f\"   Parameters: {best_params:.1f}M\")\nprint(f\"   Training Time: {best_time:.1f} minutes\")\n\nprint(f\"\\nüìà All Models Ranked:\")\nfor idx, row in comparison_df.iterrows():\n    rank_emoji = \"ü•á\" if idx == 0 else \"ü•à\" if idx == 1 else \"ü•â\" if idx == 2 else \"  \"\n    print(f\"   {rank_emoji} {idx+1}. {row['Model']}: {row['Accuracy (%)']:.2f}% (F1: {row['F1-Score']:.3f})\")\n\nprint(f\"\\nüìÅ Files Saved:\")\nprint(f\"   ‚úÖ butterfly_model_best.{model_saved}\")\nprint(f\"   ‚úÖ class_indices.json\")\nprint(f\"   ‚úÖ model_info.json\")\nprint(f\"   ‚úÖ model_comparison.csv\")\nprint(f\"   ‚úÖ model_comparison.png\")\n\nprint(f\"\\nüöÄ Next Steps:\")\nprint(f\"   1. Download all 5 files from the Output tab\")\nprint(f\"   2. Install locally: pip install tensorflow streamlit plotly\")\nprint(f\"   3. Run: streamlit run streamlit_app.py\")\nprint(f\"   4. Deploy with Docker or to cloud!\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üéì Perfect for your capstone project!\")\nprint(\"üíº Portfolio-quality machine learning project!\")\nprint(\"=\"*70)\n\nprint(f\"\\n‚ú® Your butterfly classifier is ready! ü¶ã\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:19:37.275603Z","iopub.execute_input":"2026-01-08T14:19:37.276287Z","iopub.status.idle":"2026-01-08T14:19:37.287856Z","shell.execute_reply.started":"2026-01-08T14:19:37.276225Z","shell.execute_reply":"2026-01-08T14:19:37.287133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# IT will zip kaggle/working directorry \n# !zip -r working_directory.zip /kaggle/working/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T09:40:05.017577Z","iopub.execute_input":"2026-01-09T09:40:05.017975Z","iopub.status.idle":"2026-01-09T09:40:05.022086Z","shell.execute_reply.started":"2026-01-09T09:40:05.017948Z","shell.execute_reply":"2026-01-09T09:40:05.021367Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}